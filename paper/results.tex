\subsection{Testing Methodology}

The classifier was tested by performing 10-fold cross validation on our building dataset. 
The images were loaded and their order was shuffled with a pseudo-random shuffle on a fixed seed. 
Since the images were gathered by walking around landmarks and taking images systematically, shuffling was necessary so that test partitions would not eliminate certain angles from the training set. 
The seed was fixed so that the effects of modifying the algorithm would not be masked by random variation. This allowed for fast iterative improvements. After the images were shuffled, the set was partitioned into ten equal subsets. 
For each subset, the other 9 subsets were used to train the classifier, and then each image in the chosen subset was classified as one of the 17 buildings. 
The test suite returned accuracies for the top 1, 3, and 10 nearest image neighbors.

%The first guess correctly predicted the building in the image with an average accuracy of 81.17\% (and a standard deviation of 3.08\%). 
%These results are comparable to the original retrieval accuracy on the Oxford Buildings Dataset (add actual numbers and citation), which used a 5000-image dataset of 17 buildings.

Presented in Table \ref{tab:results} we show the retrieval accuracy of our system for 1-NN, 3-NN, and 10-NN nearest neighbor accuracy classifiers.
We tested standard SIFT Feature Descriptors with varying codebook size and different feature detection algorithms.
The results provided below contain the average accuracy of each component.

\begin{table*}[ht!]
\label{tab:results}
\centering
\begin{tabular}{| c | c | c | c | c | c |}
\hline
Descriptor & Detector & Code Word Size & 1-NN & 3-NN & 10-NN\\ \hline
SIFT & SIFT & 243 & .8126 & .9081  & \textbf{.9648}  \\ \hline
SIFT & STAR & 243 & \textbf{.8711} & \textbf{.9144} & .9558 \\ \hline
SIFT & SIFT & 729 & .7981 & .8707 & .9324 \\ \hline
SIFT & STAR & 729 & .8225 & .8828 & .9288 \\ \hline
SIFT & STAR & 2187 & .7198 & .8072 & .8837\\ \hline
\end{tabular}
\vspace*{10pt}
\caption{Retrieval Accuracy}
\end{table*}

In the application, the top 30 image neighbors are returned and then then their corresponding landmarks are returned as possible candidates. In practice, a user typically sees two to four candidate landmarks.

\subsection{Per-landmark Accuracy}

Accuracies were also recorded for each of the seventeen landmarks.

table goes here:

0.53: alma_mater
0.61: thinker
0.69: lerner
0.66: CEPSR
0.70: lion
0.72: curl
0.75: mudd
0.77: pan
0.84: havemayer
0.84: butler
0.85: journalism
0.91: mathematics
0.95: noco
0.97: hamilton
0.98: low
0.98: law
0.99: pupin

Recording these accuracies serves two purposes. First, it diagnoses which buildings may require more training data. Second, when the application is scaled to more landmarks, these accuracies will dictate when a landmark is ready for production. For example, a landmark can be added to the client program only when its accuracy exceeds 80%. For the results reported here, we neither selectively gathered data nor culled poorly-performing landmarks. However, these strategies will be some of the first steps taken when improving the application for future versions.  
