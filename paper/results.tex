The classifier was tested by performing 10-fold cross validation on our building dataset. 
The images were loaded and their order was shuffled with a psuedorandom shuffle on a fixed seed. 
Since the images were gathered by walking around landmarks and taking images systematically, shuffling was necessary so that test partitions would not eliminate certain angles from the training set. 
The seed was fixed so that the effects of modifying the algorithm would not be masked by random variation. This allowed for fast iterative improvements. After the images were shuffled, the set was partitioned into ten equal subsets. 
For each subset, the other 9 subsets were used to train the classifer, and then  each image in the chosen subset was classified as one of the 17 buildings. 
The test suite returned accuracies measuring when the correct building was in the top 1, 3, and 10 guesses. (results below)

%The first guess correctly predicted the building in the image with an average accuracy of 81.17\% (and a standard deviation of 3.08\%). 
%These results are comparable to the original retreival accuracy on the Oxford Buildings Dataset (add actual numbers and citation), which used a 5000-image dataset of 17 buildings.

Presented in Table \ref{tab:results} we show the retrieval accuracy of our system for 1-NN, 3-NN, and 10-NN nearest neighbor accuracy classifiers.
We tested standard SIFT Feature Descriptors with varying codebook size and differnet feature detection algorithms.
The results provided below contain the average accuracy of each componenet.

\begin{table}[h]
\begin{center}
\caption{Retrieval Accuracy}
\label{tab:results}
\begin{tabular}{| c | c | c | c | c | c |}
\hline
Descriptor & Detector & Code Word Size & 1-NN & 3-NN & 10-NN\\ \hline
SIFT & SIFT & 729 & Num & Num & Num \\ \hline
SIFT & STAR & 729 & Num & Num & Num \\ \hline
SIFT & SIFT & 2187 & Num & Num & Num \\ \hline
SIFT & STAR & 2187 & Num & Num & Num\\ \hline
\end{tabular}
\end{center}
\end{table}
