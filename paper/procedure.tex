Our object classification procedure can be broken down into 3 stages: feature
extraction, codebook generation, and \emph{k}-d tree construction.  This process is described in Figures 2 and 3.

\subsection{Feature Extraction}
First, points of interest were detected in the image using OpenCV's StarDetector,
a variation of the CenSurE keypoint detector \cite{agrawal2008censure}. 
The CenSurE keypoint detector is a robust keypoint detector that is both fast and returned suitable points for image retrieval and matching.
Its detection speed over the SIFT detector was important in this application due to the need for quick response from our system to the user.
To extract features at these keypoints we use the well-known SIFT algorithm \cite{lowe1999object}. 
SIFT was chosen for its strong invariance under lighting conditions and scale which is important in the task of analyzing unpredictable user input. 
Additionally, these algorithms were chosen because they performed best in our initial experiments.

\subsection{Codebook Generation}
To quantize the SIFT feature space, a codebook with \emph{k} words was constructed through the use of the k-means algorithm.
Once we had a set of SIFT feature vectors for each image in our training set, \emph{k}-means was applied to determine \emph{k} cluster centers in the vector space. 
This set of cluster centers in the SIFT feature space was used as the codebook. 
A kd-tree was then constructed using OpenCV's FLANN \cite{muja2009fast} package to index the codebook so that it could be queried quickly. 
At this point, we had a data structure to efficiently map a SIFT feature vector to a code word.

We experimented using different sizes of codebooks within our experiments, and the results can be seen in Table \ref{tab:results}.

\subsection{K-d Tree Construction and BoW}
Using this codebook, a histogram of word occurrences was constructed for each training image. 
We use a FLANN \emph{k}-d tree to index the features for each image into a histogram.
This is known as a Bag of Words image representation and is a popular and state of the art method for image retrieval and classification.
For each feature within the image we send the features down our \emph{k}-d tree, and then sum the number of times a feature maps to a respective leaf node or code word for an image.
We are then left with an unnormalized histogram representation of features within each image.
We then normalize the images so that the sum of the histograms of each image is the same.
Finally, we implement a TF-IDF weighting scheme to maximize the information that is available within each image histogram.
TF-IDF is a commonly used technique in text processing to weight the important words within a document retrieval scheme.  
Words that appear in many documents, such as ``and'' and ``the'' receive very low weights, and words that are more subject and document specific receive higher weights in the classification scheme.
We use this idea for our image retrieval system utilizing our learned codebook.
We weight each dimension of our BoW histogram representation using equation \ref{eq:TF-IDF},
\begin{equation}
\label{eq:TF-IDF}
w_{i} = ln\frac{N}{N_{i}},
\end{equation}
where $w_i$ are the weight vector for the words, $N$ is the number of images in the dataset, and $N_i$ is the number of images in the dataset that contain codeword $i$.
FLANN was used again to construct an index of these histograms. 
We now have a data structure to efficiently map an unknown histogram to a known histogram.
In both vector spaces, euclidean distance is used as the distance metric. 
This decision was made based on experimentation.

Given a test image, the classification process can be summarized in the following steps:
\begin{enumerate}
\item Extract the set of SIFT features (vectors of length 128).

\item Map SIFT features to pre-defined codewords

\item Bin the extracted SIFT features together to create a histogram representation of an image

\item Find the ``k'' closest matching histograms in the training set using euclidean distance.

\item Return the class of the matching histograms.
\end{enumerate}




